> [!CAUTION]
> ### PROGRESS PAUSED
> I am not actively working on this, and don't remember it's status except for that it kinda worked but then got janky when I tried adding on-demand simulated outputs. Might revisit later. It can probably become what it was intended to become with the improved vibe coding as of Aug 2025


# Plan (not actual status)

- Side by side comparison of what "tokens per second" outputs from LLMs look like.
- Output realistic but simulated outputs (but I had issues getting it to do that on the fly)
  - Probably would make more sense to create a feature that allowed you to paste in the desired output
  - Or make some "presets" like
    - Code
    - Multi-step technical w/ multiple code blocks
    - Well formatted non-code markdown article
    - Long-ass "Deep Research"
  -  Or actually prompt an LLM with streaming off, then fake stream the results side by side at the desired speeds
- Offer Statistics like:
  - Total time to finish output
  - Time to fill visible area of chat
  - What multiplier the output is to average reading speed (allow user to define)
  
